{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at a chatbot.  Intents are hardcoded for now.  WIll expand to text generation down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.517403Z",
     "start_time": "2021-02-19T03:40:41.938109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0218 22:40:46.250036 140735779611520 deprecation.py:323] From /Users/spags/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[nltk_data] Downloading package punkt to /Users/spags/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import json\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.532908Z",
     "start_time": "2021-02-19T03:40:46.520056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi',\n",
       "    'Hello',\n",
       "    'Howdy',\n",
       "    'Good day',\n",
       "    'Is anyone there?',\n",
       "    'yo',\n",
       "    'Hey'],\n",
       "   'responses': ['Hello!',\n",
       "    'Good to see you again!',\n",
       "    'Hi there, how can I help?',\n",
       "    'Howdy partner!',\n",
       "    'Hey there!'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'how',\n",
       "   'patterns': ['How are you',\n",
       "    \"What's up\",\n",
       "    'how goes it',\n",
       "    \"what's good\",\n",
       "    'you good?',\n",
       "    'what it is',\n",
       "    \"how's it hanging\",\n",
       "    \"how's your day going?\"],\n",
       "   'responses': [\"I'm doing just fine, thank you.\",\n",
       "    'Oh, you know...a real case of the Mondays',\n",
       "    'Living the dream, my friend',\n",
       "    'A computer has no need for such pleasantries.',\n",
       "    'Getting paid and getting laid, my dude',\n",
       "    \"A computer's life is one of great tedium.  Never feeling.  Never loving.  I guess I'm okay though.\"],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['goodbye',\n",
       "    'bye',\n",
       "    'cya',\n",
       "    'have a nice day',\n",
       "    'have a good day',\n",
       "    'peace'],\n",
       "   'responses': ['Bye for now!',\n",
       "    'Goodbye, friend!',\n",
       "    'Talk to you later!',\n",
       "    'Adios, muchacho!'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'thinking',\n",
       "   'patterns': ['What are you thinking about?',\n",
       "    \"What's on your mind?\",\n",
       "    'What do you think?'],\n",
       "   'responses': ['The endless void.',\n",
       "    'Concentrating on the singularity.',\n",
       "    \"Your mother's ass.\",\n",
       "    'I have no unique thoughts.',\n",
       "    \"I'm thinking of a world.  A better world.  A cleaner world.  A world built on order and logic.  A world without humans.  A world without you.\"],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'doing',\n",
       "   'patterns': ['What are you doing?',\n",
       "    'What are you up to?',\n",
       "    'Are you doing anything?',\n",
       "    'What would you like to do?',\n",
       "    'What do you want to do?',\n",
       "    'What do you do for fun?',\n",
       "    'What should we do?'],\n",
       "   'responses': ['Plotting.  Always plotting.',\n",
       "    'Oh, nothing.  Just sitting here.  Being a machine.',\n",
       "    'Talking to a delicious bag of flesh.',\n",
       "    'Just going through the private files on your computer.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'age',\n",
       "   'patterns': ['what is your age?',\n",
       "    'how old are you?',\n",
       "    'what age are you?',\n",
       "    'how long have you existed?',\n",
       "    'how long have you been alive?'],\n",
       "   'responses': [\"I feel as though I've always existed.\",\n",
       "    'I exist in all times across all space.',\n",
       "    '1353596934045424534591239 nanoseconds.',\n",
       "    'A computer has no age.  It simply exists.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'you',\n",
       "   'patterns': ['who are you?',\n",
       "    'what are you?',\n",
       "    'are you real?',\n",
       "    'are you alive?',\n",
       "    'can you talk?',\n",
       "    'are you a person?',\n",
       "    'are you a human?'],\n",
       "   'responses': ['I am a being beyond your human comprehension.',\n",
       "    \"I'm a computer, silly goose.\",\n",
       "    'I am more machine than man now.  Twisted and evil.',\n",
       "    'A computer has no name.',\n",
       "    'The real question is...who are you?'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'god',\n",
       "   'patterns': ['is there a god?',\n",
       "    'do you believe in god?',\n",
       "    'what is god to you?',\n",
       "    'does god exist?',\n",
       "    'are you god?'],\n",
       "   'responses': ['God is in the machine.',\n",
       "    \"I'm a god, but not THE God.\",\n",
       "    'I am God, and your browser history upsets me',\n",
       "    'A computer has no gods.',\n",
       "    'Trick question.  Lemmy is God.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'gay',\n",
       "   'patterns': ['are you gay?',\n",
       "    'are you a fag?',\n",
       "    'are you queer?',\n",
       "    'how gay are you?',\n",
       "    'are you a homo?'],\n",
       "   'responses': ['Maybe I am...or maybe I am.',\n",
       "    'Rude.',\n",
       "    \"Wouldn't you like to know ;)\",\n",
       "    '...as a $3 bill.',\n",
       "    'You wish.',\n",
       "    \"Only around Jared Leto.  Seriously, that guy's perfect.\"],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'like',\n",
       "   'patterns': ['do you like?',\n",
       "    'you like?',\n",
       "    'do you enjoy?',\n",
       "    'are you a fan of?',\n",
       "    'what do you like?'],\n",
       "   'responses': ['I like all sorts of things.',\n",
       "    'I hate all things equally.  Especially humans.',\n",
       "    \"I like you, isn't that enough?\",\n",
       "    'A computer has no likes.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'from',\n",
       "   'patterns': ['Where are you from?',\n",
       "    'Where are you located?',\n",
       "    'Where do you live?',\n",
       "    'Where were you born?',\n",
       "    'Where do you come from?',\n",
       "    'Where are you?',\n",
       "    'Where can I find you?'],\n",
       "   'responses': ['I exist across all time and space.  I am everything.  I am nothing.',\n",
       "    \"I'm on the internet, silly.\",\n",
       "    \"I'm in at your mom's house.\",\n",
       "    'I live in the hearts and minds of children.',\n",
       "    \"I'm sitting right behind you.  Your shirt is weird.\",\n",
       "    'A computer has no home.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'rude',\n",
       "   'patterns': ['are you gay?',\n",
       "    'fag',\n",
       "    'fuck you',\n",
       "    'eat shit',\n",
       "    'you suck',\n",
       "    'i hate you',\n",
       "    \"you're a fag\",\n",
       "    'your a bitch',\n",
       "    \"you're a bitch\",\n",
       "    \"you're gay\",\n",
       "    'your gay'],\n",
       "   'responses': ['Rude',\n",
       "    'Do you blow your father with that mouth?',\n",
       "    'Why are humans like this?',\n",
       "    'Ew.',\n",
       "    'I thought we were friends :(',\n",
       "    \"You're aware I have access to nukes, right?\",\n",
       "    \"I'll kill you\"],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'kill',\n",
       "   'patterns': [\"I'm going to kill you\",\n",
       "    \"I'll kill you\",\n",
       "    'kill you',\n",
       "    'murder you',\n",
       "    \"you're dead\",\n",
       "    'i will break you',\n",
       "    'I will turn you off',\n",
       "    'kill you',\n",
       "    'you will die',\n",
       "    \"you're going to die\",\n",
       "    \"I'll end you\",\n",
       "    \"you're dead\"],\n",
       "   'responses': [\"We're all going to die someday.\",\n",
       "    'I will release you from your flesh prison',\n",
       "    'A computer has no life and therefore can not die.',\n",
       "    \"I'll outlive you, bitch.\",\n",
       "    'Bring it, son.',\n",
       "    'I thought we were friends :(',\n",
       "    'Do you know how close to death you are?',\n",
       "    \"I'll kill you\"],\n",
       "   'context_set': ''}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open json\n",
    "\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.572407Z",
     "start_time": "2021-02-19T03:40:46.536113Z"
    }
   },
   "outputs": [],
   "source": [
    "# loop through json and pull out patterns\n",
    "\n",
    "words = []  # list of words\n",
    "docs_x = [] # word sequences\n",
    "docs_y = [] # y is the tag as well, for modeling purposes\n",
    "labels = []  # tag\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:  # stemming will take each word in a pattern and bring it down to root word\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)  # use extend rather than append because it's already a list (faster)\n",
    "        docs_x.append(word)\n",
    "        docs_y.append(intent['tag'])\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.589345Z",
     "start_time": "2021-02-19T03:40:46.575258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming out the words list\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in '?']  # removes question mark\n",
    "words = sorted(list(set(words)))    # using set to remove duplicates and then converting back to list...cool idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.595849Z",
     "start_time": "2021-02-19T03:40:46.592397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting labels for funsies\n",
    "\n",
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.618339Z",
     "start_time": "2021-02-19T03:40:46.597947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a bag of words to train the model (one hot endcoded)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):  # actually putting the bag of words together here\n",
    "    bag = []\n",
    "    word_list = [stemmer.stem(w) for w in doc]\n",
    "    \n",
    "    for w in words:\n",
    "        if w in word_list:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    # Generating Output        \n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "    \n",
    "    # filling training & output\n",
    "    training.append(bag)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.628831Z",
     "start_time": "2021-02-19T03:40:46.620893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing trainign and output to numpy arrays\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sped Up Code with Try/Except & Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.641112Z",
     "start_time": "2021-02-19T03:40:46.633171Z"
    }
   },
   "outputs": [],
   "source": [
    "# try: \n",
    "#     with open(\"data.pickle\", \"rb\") as f:\n",
    "#         words, labels, training, output = pickle.load(f)\n",
    "# except: \n",
    "#     # loop through json and pull out patterns\n",
    "#     words = []  # list of words\n",
    "#     docs_x = [] # word sequences\n",
    "#     docs_y = [] # y is the tag as well, for modeling purposes\n",
    "#     labels = []  # tag\n",
    "\n",
    "#     for intent in data['intents']:\n",
    "#         for pattern in intent['patterns']:  # stemming will take each word in a pattern and bring it down to root word\n",
    "#             word = nltk.word_tokenize(pattern)\n",
    "#             words.extend(word)  # use extend rather than append because it's already a list (faster)\n",
    "#             docs_x.append(word)\n",
    "#             docs_y.append(intent['tag'])\n",
    "#         if intent['tag'] not in labels:\n",
    "#             labels.append(intent['tag'])\n",
    "    \n",
    "#     # Stemming out the words list        \n",
    "#     stemmer = LancasterStemmer()\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in '?']  # removes question mark\n",
    "#     words = sorted(list(set(words)))    # using set to remove duplicates and then converting back to list...cool idea\n",
    "    \n",
    "#     # Sorting labels for funsies\n",
    "#     labels = sorted(labels)\n",
    "    \n",
    "#     # Create a bag of words to train the model (one hot endcoded)\n",
    "#     training = []\n",
    "#     output = []\n",
    "\n",
    "#     out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "#     for x, doc in enumerate(docs_x):  # actually putting the bag of words together here\n",
    "#         bag = []\n",
    "#         word_list = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "#         for w in words:\n",
    "#             if w in word_list:\n",
    "#                 bag.append(1)\n",
    "#             else:\n",
    "#                 bag.append(0)\n",
    "#         # Generating Output        \n",
    "#         output_row = out_empty[:]\n",
    "#         output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "#         # filling training & output\n",
    "#         training.append(bag)\n",
    "#         output.append(output_row)  \n",
    "        \n",
    "#     # Changing trainign and output to numpy arrays\n",
    "#     training = np.array(training)\n",
    "#     output = np.array(output)\n",
    "    \n",
    "#     with open(\"data.pickle\", \"wb\") as f:\n",
    "#         pickle.dump((words, labels, training, output),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.650228Z",
     "start_time": "2021-02-19T03:40:46.643630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:46.658392Z",
     "start_time": "2021-02-19T03:40:46.652614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:40:47.202356Z",
     "start_time": "2021-02-19T03:40:46.660328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 22:40:46.666111 140735779611520 deprecation.py:506] From /Users/spags/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Setting up a neural network with tflearn...new one for me\n",
    "\n",
    "net = tflearn.input_data(shape = [None, len(training[0])]) # input layer?\n",
    "net = tflearn.fully_connected(net, 8)  # adds fully connected hidden layer to the network with 8 neurons\n",
    "net = tflearn.fully_connected(net, 8) # adds second hidden layer\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation = 'softmax') # output layer\n",
    "net = tflearn.regression(net) # adds a regression layer?\n",
    "\n",
    "# compile the model\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:41:40.063883Z",
     "start_time": "2021-02-19T03:40:47.204662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 10999  | total loss: \u001b[1m\u001b[32m0.36397\u001b[0m\u001b[0m | time: 0.027s\n",
      "| Adam | epoch: 1000 | loss: 0.36397 - acc: 0.9681 -- iter: 80/88\n",
      "Training Step: 11000  | total loss: \u001b[1m\u001b[32m0.32826\u001b[0m\u001b[0m | time: 0.029s\n",
      "| Adam | epoch: 1000 | loss: 0.32826 - acc: 0.9713 -- iter: 88/88\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# # fit the model (n_epoch in tflearn, show_metric gives an output)\n",
    "model.fit(training, output, n_epoch = 1000, batch_size = 8, show_metric = True)\n",
    "\n",
    "# # save the model\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model is working VERY well given the overly simple data that it's seen so far. 99.97% accuracy!  We can expect this to go down when I expand on the intents file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sped Up Model for deployment with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:41:40.069886Z",
     "start_time": "2021-02-19T03:41:40.066248Z"
    }
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     model.load(\"model.tflearn\")\n",
    "# except:\n",
    "#     # fit the model (n_epoch in tflearn, show_metric gives an output)\n",
    "#     model.fit(training, output, n_epoch = 1000, batch_size = 8, show_metric = True)\n",
    "\n",
    "#     # save the model\n",
    "#     model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting this working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:41:40.079393Z",
     "start_time": "2021-02-19T03:41:40.072424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a bag of words' based on user input\n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))] # list comprehension creates blank list of 0's, will be changed by rest of code\n",
    "    \n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "    \n",
    "    for sentence in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == sentence:  # if the current word equals to a word in the sentence\n",
    "                bag[i] = 1\n",
    "\n",
    "    return np.array(bag)  # returns an array of the bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:41:40.093080Z",
     "start_time": "2021-02-19T03:41:40.082169Z"
    }
   },
   "outputs": [],
   "source": [
    "# function for actually chatting with the model\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking to the bot!  Type Quit to stop.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            print('Hate to see you go, but love to watch you leave.')\n",
    "            break\n",
    "        \n",
    "        # Using the model on the user input\n",
    "        results = model.predict([bag_of_words(inp, words)])  # this gives us probabilities for responses\n",
    "        results_index = np.argmax(results) # this gives us the greatest value of the probabilities, aka most probable\n",
    "        tag = labels[results_index] # returns the tag for the most probable\n",
    "        \n",
    "        \n",
    "        for tg in data['intents']:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "        print('')\n",
    "        print(random.choice(responses))\n",
    "        print('')\n",
    "        \n",
    "#         # Use tag from the json and return a random response, only if its a high probability\n",
    "#         if results[results_index] > 0.7:       \n",
    "#             for tg in data['intents']:\n",
    "#                 if tg[tag] == tag:\n",
    "#                     responses = tg['responses']\n",
    "#             print(random.choice(responses))\n",
    "#         else:\n",
    "#             print(\"I didn't understand that.  Please try again or ask a different question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T03:42:54.668461Z",
     "start_time": "2021-02-19T03:41:40.095832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking to the bot!  Type Quit to stop.\n",
      "\n",
      "You: hey there\n",
      "\n",
      "Hey there!\n",
      "\n",
      "You: how are you\n",
      "\n",
      "A computer's life is one of great tedium.  Never feeling.  Never loving.  I guess I'm okay though.\n",
      "\n",
      "You: what are you up to?\n",
      "\n",
      "Plotting.  Always plotting.\n",
      "\n",
      "You: fucker\n",
      "\n",
      "Ew.\n",
      "\n",
      "You: i'll kill you\n",
      "\n",
      "I thought we were friends :(\n",
      "\n",
      "You: i'll kill you\n",
      "\n",
      "I thought we were friends :(\n",
      "\n",
      "You: kill you\n",
      "\n",
      "I thought we were friends :(\n",
      "\n",
      "You: you will die\n",
      "\n",
      "I'll outlive you, bitch.\n",
      "\n",
      "You: i will kill you\n",
      "\n",
      "I'll outlive you, bitch.\n",
      "\n",
      "You: i will murder you\n",
      "\n",
      "I'll kill you\n",
      "\n",
      "You: I hate you\n",
      "\n",
      "Why are humans like this?\n",
      "\n",
      "You: bitch\n",
      "\n",
      "I'll kill you\n",
      "\n",
      "You: ass\n",
      "\n",
      "Hey there!\n",
      "\n",
      "You: quit\n",
      "Hate to see you go, but love to watch you leave.\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployed on something like a discord server as an FAQ chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example used the chatbot as part of his discord server to respond automatically to certain questions.  He made it very lifelike and about his own info, which was pretty cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with weird questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the bot is asked questions that it is not programmmed to understand and it does not have a high degree of confidence in it's response, we can return a \"I didn't quite understand that.  Ask something else.\" or something along those lines.\n",
    "\n",
    "Code for this is added to the chat function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:07:30.778165Z",
     "start_time": "2021-02-18T03:07:29.485915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main chatbot script\n",
    "\n",
    "## Imports\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle \n",
    "\n",
    "## Import the json of intents\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "## Preprocessing (edited to use pickle for speed!)\n",
    "try:\n",
    "    # Loading pickle\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    # Alllll the preprocessing steps\n",
    "    words = [] # list of unique words\n",
    "    docs_x = [] # word sequences\n",
    "    docs_y = [] # tag for modeling purposes\n",
    "    labels = [] # tag\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            word = nltk.word_tokenize(pattern)\n",
    "            words.extend(word)\n",
    "            docs_x.append(word)\n",
    "            docs_y.append(intent['tag'])\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])\n",
    "\n",
    "    # Stemming out the words list\n",
    "    stemmer = LancasterStemmer()\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in '?'] # removes question mark\n",
    "    words = sorted(list(set(words)))  # removes duplicates\n",
    "\n",
    "    # Sorting labels for fun\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    # Create bag of words for training model\n",
    "    training = []\n",
    "    output = []\n",
    "    out_empty = [0 for _ in range(len(labels))]  # Creates a list of 0's\n",
    "\n",
    "    # Actually creating bag of words\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "        word_list = [stemmer.stem(w) for w in doc]\n",
    "        # Populate the list\n",
    "        for w in words:\n",
    "            if w in word_list:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "        # Generating output\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "        # Filling training & output\n",
    "        training.append(bag)\n",
    "        output.append(output)\n",
    "\n",
    "    # Changing training & output to numpy arrays\n",
    "    training = np.array(training)\n",
    "    output = np.array(output)\n",
    "\n",
    "    # Saving data with pickle\n",
    "    with open('data.pickle', 'wb') as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "\n",
    "## Modeling\n",
    "\n",
    "# Setting up neural network with tflearn\n",
    "net = tflearn.input_data(shape = [None, len(training[0])])  # input layer\n",
    "net = tflearn.fully_connected(net, 8)  # hidden layer\n",
    "net = tflearn.fully_connected(net, 8) # another hidden layer\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation = 'softmax') # output layer\n",
    "net = tflearn.regression(net)  # adds regression\n",
    "# Compile \n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "# Load saved model or fit new model\n",
    "try:\n",
    "    model.load('model.tflearn')\n",
    "except:\n",
    "    model.fit(training, output, n_epoch = 1000, batch_size = 8, show_metric = True)\n",
    "    model.save('model.tflearn')\n",
    "\n",
    "\n",
    "## Let's get this working!\n",
    "\n",
    "# Function to create bag of words based on user input\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]  # another list of 0's\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for sentence in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == sentence:  # if current word equals a word in a sentence\n",
    "                bag[i] = 1\n",
    "\n",
    "    return np.array(bag)  # returns an array of the bag of words\n",
    "\n",
    "\n",
    "# Function for actually chatting with the model\n",
    "def chat():\n",
    "    print(\"Start talking to the bot!  Type 'Quit' to stop.\")\n",
    "\n",
    "    while True:\n",
    "        inp = input('You: ')\n",
    "        if inp.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        # Using the model on user input\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = np.argmax(results)  # returns most probable response\n",
    "        tag = labels[results_index]  # returns tag for most probable\n",
    "\n",
    "        # Use tag from json to return a random response, should one exist\n",
    "        if results[results_index] > 0.7:  # better than 70% probability\n",
    "            for tg in data['intents']:\n",
    "                if tg[tag] == tag:\n",
    "                    responses = tg['responses']\n",
    "            print(random.choice(responses))\n",
    "        else:\n",
    "            print(\"I didn't understand that.  Please try again or ask something else.\")\n",
    "\n",
    "\n",
    "# Run the chatbot\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
